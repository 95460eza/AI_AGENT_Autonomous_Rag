{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0- LIBRARIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "import ast\n",
    "\n",
    " \n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "import asyncio\n",
    " \n",
    "from transformers import MistralForCausalLM\n",
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "\n",
    "from mistralai import Mistral \n",
    "\n",
    "from mistral_inference.transformer import Transformer\n",
    "from mistral_inference.generate import generate\n",
    "from mistral_common.tokens.tokenizers.mistral import MistralTokenizer \n",
    "from mistral_common.protocol.instruct.messages import UserMessage\n",
    "from mistral_common.protocol.instruct.request import ChatCompletionRequest\n",
    "\n",
    "from llama_index.core.settings import Settings\n",
    "from llama_index.llms.mistralai import MistralAI\n",
    "from llama_index.core import SimpleDirectoryReader, SummaryIndex, VectorStoreIndex\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.core.llms.custom import CustomLLM\n",
    "from llama_index.core.tools import QueryEngineTool\n",
    "from llama_index.core.query_engine.router_query_engine import RouterQueryEngine\n",
    "from llama_index.core.selectors import LLMSingleSelector\n",
    "from llama_index.core.tools import FunctionTool, ToolMetadata\n",
    "from llama_index.core.vector_stores import MetadataFilters, FilterCondition\n",
    "from llama_index.core.objects import ObjectIndex\n",
    "from llama_index.core.agent import FunctionCallingAgentWorker\n",
    "from llama_index.core.agent import AgentRunner\n",
    "\n",
    "from llama_index.core.llms import ChatMessage, ChatResponse, MessageRole, CompletionResponse \n",
    " \n",
    "from llama_index.core.llms.function_calling import FunctionCallingLLM\n",
    " \n",
    "from llama_index.core.agent.workflow.workflow_events import ToolCall\n",
    "\n",
    "\n",
    "\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import Any, Optional, List, Sequence\n",
    " \n",
    " \n",
    "\n",
    "\n",
    "import importlib\n",
    "import utilities.utils\n",
    "importlib.reload(utilities.utils)\n",
    "from utilities.utils import get_doc_tools\n",
    "\n",
    "\n",
    "from pathlib import Path\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I- URLs FOR THE KNOWLEDGE BASE IN \"PDF\" FORMAT\n",
    "\n",
    "* SAVE THE PDF LISTED BELOW IN A FOLDER CALLED \"llamaindex_datasets\" INSIDE THE CURRENT WORKING DIRECTORY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAVE THE PDF LISTED BELOW IN A FOLDER CALLED \"llamaindex_datasets\" INSIDE THE CURRENT WORKING DIRECTORY\n",
    "\n",
    "# urls = [\n",
    "#     \"https://openreview.net/pdf?id=VtmBAGCN7o\",\n",
    "#     \"https://openreview.net/pdf?id=6PmJoRfdaK\",\n",
    "#     \"https://openreview.net/pdf?id=LzPWWPAdY4\",\n",
    "#     \"https://openreview.net/pdf?id=VTF8yNQM66\",\n",
    "#     \"https://openreview.net/pdf?id=hSyW5go0v8\",\n",
    "#     \"https://openreview.net/pdf?id=9WD9KwssyT\",\n",
    "#     \"https://openreview.net/pdf?id=yV6fD7LYkF\",\n",
    "#     \"https://openreview.net/pdf?id=hnrB5YHoYu\",\n",
    "#     \"https://openreview.net/pdf?id=WbWtOYIzIK\",\n",
    "#     \"https://openreview.net/pdf?id=c5pwL0Soay\",\n",
    "#     \"https://openreview.net/pdf?id=TpD2aG1h0D\"\n",
    "# ]\n",
    "\n",
    "papers = [\n",
    "    \"metagpt.pdf\",\n",
    "    \"longlora.pdf\",\n",
    "    \"loftq.pdf\",\n",
    "    \"swebench.pdf\",\n",
    "    \"selfrag.pdf\",\n",
    "    \"zipformer.pdf\",\n",
    "    \"values.pdf\",\n",
    "    \"finetune_fair_diffusion.pdf\",\n",
    "    \"knowledge_card.pdf\",\n",
    "    \"metra.pdf\",\n",
    "    \"vr_mcl.pdf\"\n",
    "]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# II- SET UP \"MISTRAL AI\" AS OUR LLM "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II.1. - DOWNLOAD (IF NOT ON YOUR LOCAL MACHINE) THE MISTRAL LLM WEIGHTS AND TOKENIZER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hugging Face token successfully retrieved.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68142da414974d4bb0421c55edfacecf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 6 files:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MISTRAL MODEL WEIGHTS DOWNLOADED\n"
     ]
    }
   ],
   "source": [
    "# TO DOWNLOAD MODEL WEIGHTS \"DIRECTLY\" FROM \"HUGGING FACE\":\n",
    "\n",
    "# RUNNING \"MISTRAL-7B\" IN GGUF \"FORMAT\" REQUIRES EITHER \"LlamaCpp\" INSTALLED OR USING \"MISTRAL-INFERENCE\" THE \"INFERENCE\" \"BACKEND\" PROVIDED BY MISTRALAI  \n",
    "# NOTE 1: TO INSTALL \"LlamaCpp\", RUN: CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\" pip install llama-cpp-python --force-reinstall --upgrade --no-cache-dir\n",
    "# NOTE 2: \"MISTRAL-INFERENCE\" IS A \"RUST-BACKED\" AND \"CUDA-ACCELERATED\" INFERENCE ENGINE THAT SUPPORTS \"GGUF\" FORMAT \"NATIVELY\". \n",
    "# FOR \"NVIDIA\" GPU SETUPS, IT IS MORE EFFICIENT BECAUSE IT USES \"FLASHATTENTION\", TENSOR \"PARALLELISM\" AND \"TORCH-STYLE\" CUDA BINDINGS THUS PROVIDING\n",
    "# LOW-LATENCY INFERENCE, FATS-TOKEN GENERATION. YOU ACCESS IT VIA \"Transformer.from_folder(model_path)\" AND INFERENCE IS DONE BY CALLING THE \"GENERATE\" \n",
    "# CLASS FROM \"mistral_inference\"\n",
    "\n",
    "hf_token = os.getenv('HUGFACE_AUTH_TOKEN')\n",
    "\n",
    "if hf_token is None:\n",
    "    raise ValueError(\"Hugging Face token not found. Please set it as an environment variable.\")\n",
    "else:\n",
    "    print(\"Hugging Face token successfully retrieved.\")\n",
    "\n",
    "\n",
    "model_path = os.path.join(os.getcwd(), 'model_weights')\n",
    "\n",
    "\n",
    "repo_id = \"mistralai/Mistral-Small-3.1-24B-Instruct-2503\"\n",
    "snapshot_download(repo_id= repo_id,   \n",
    "                   allow_patterns=[\"params.json\", # MODEL CONFIGURATION PARAMETERS\n",
    "                                  \"config.json\",\n",
    "                                  \"consolidated.safetensors\",  \n",
    "                                  \"tokenizer.json\", # MODEL TOKENIZER DATA  (vocab, merges, etc.).\n",
    "                                   \"tokenizer_config.json\", #  CONFIGURATION INFO LIKE SPECIAL TOKENS AND OTHER SETTINGS \n",
    "                                    \"tekken.json\" # MISTRALAI MOST ADVANCED TOKENIZER  \n",
    "                                   ],  \n",
    "                   local_dir=model_path,  # Local directory to save the files\n",
    "                   use_auth_token= hf_token,  # Use authentication token\n",
    "                   #local_dir_use_symlinks=False  # optional: avoids symlinks, stores real files\n",
    "                )\n",
    "\n",
    "print(\"MISTRAL MODEL WEIGHTS DOWNLOADED\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "tokenizer = MistralTokenizer.from_file(f\"{model_path}/tekken.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MISTRAL MODEL LOADED\n"
     ]
    }
   ],
   "source": [
    "# LOAD THE \"MISTRAL SMALL\" LLM MODEL\n",
    "\n",
    "mistral_llm = Transformer.from_folder(model_path)\n",
    "\n",
    "print(\"MISTRAL MODEL LOADED\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II.2. - CREATE A LLAMAINDEX \"WRAPPER\" TO \"INTEGRATE\" THE LLM WITH THE LLAMAINDEX FRAMEWORK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper functions\n",
    "\n",
    "def extract_json_blocks(text: str):\n",
    "    pattern = r\"```json\\s*(\\{.*?\\})\\s*```\"\n",
    "    matches = re.findall(pattern, text, re.DOTALL)\n",
    "    return [json.loads(m) for m in matches]\n",
    "\n",
    "def string_to_object_in_quotes(string_of_python_object):\n",
    "    try:\n",
    "        return ast.literal_eval(string_of_python_object)\n",
    "    except (SyntaxError, ValueError) as e:\n",
    "        print(f\"Error converting string to python object: {e}\")\n",
    "        print(string_of_python_object)\n",
    "        return string_of_python_object\n",
    "\n",
    "target_words = r'(\"choice\":|\"reason\":)'\n",
    "word2 = \"reason\"\n",
    "\n",
    "def split_on_keywords(splitting_words, text):\n",
    "    result_dict = {}\n",
    "    parts = re.split(splitting_words, text)\n",
    "    \n",
    "    if len(parts) > 1:\n",
    "        for i in range(1, len(parts), 2):\n",
    "            key = parts[i].strip()\n",
    "            value = parts[i+1].strip() if i+1 < len(parts) else \"\"\n",
    "            \n",
    "            if string_to_object_in_quotes(key[0:-1]) == word2:\n",
    "                result_dict[string_to_object_in_quotes(key[0:-1])] = (\n",
    "                    string_to_object_in_quotes(value.replace(\"\\n\", \"\").replace(\"]\", \"\").replace(\"}\", \"\")\n",
    "                                               .replace(\"\\\\\", \"!\").replace(\",\", \"\").replace(\"{\", \"\"))\n",
    "                )\n",
    "            else:\n",
    "                result_dict[string_to_object_in_quotes(key[0:-1])] = int(value.strip(','))\n",
    "    else:\n",
    "        result_dict['text'] = text\n",
    "    return result_dict\n",
    "\n",
    "# JSON storage\n",
    "class JSONStorage:\n",
    "    def __init__(self, data):\n",
    "        self.text = data\n",
    "\n",
    "# Metadata class\n",
    "class LLMMetadata:\n",
    "    def __init__(self, model_name, model_type, context_window, version, num_output=1):\n",
    "        self.model_name = model_name\n",
    "        self.model_type = model_type\n",
    "        self.context_window = context_window\n",
    "        self.version = version\n",
    "        self.num_output = num_output\n",
    "        self.is_chat_model = True\n",
    "        self.is_function_calling_model = True\n",
    "\n",
    "# MAIN WRAPPER\n",
    "# If using FunctionCallingLLM, these 3 \"METHODS\" must be implemented: \".get_tool_calls_from_response()\", \".predict_and_call(...)\" AND \".predict(...)\"\n",
    "class MistralLlamaIndexWrapper(FunctionCallingLLM, BaseModel):\n",
    "    tokenizer: Any\n",
    "    llm: Any\n",
    "    max_tokens: int = Field(description=\"Maximum tokens to generate\")\n",
    "\n",
    "    def __init__(self, llm: Any, tokenizer: Any, max_tokens: int, **kwargs):\n",
    "        super().__init__(llm=llm, tokenizer=tokenizer, max_tokens=max_tokens, **kwargs)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.llm = llm\n",
    "        self.max_tokens = max_tokens\n",
    "\n",
    "    def _messages_to_prompt(self, messages: List[ChatMessage]) -> str:\n",
    "        \"\"\"Convert a list of ChatMessage objects into a single prompt string.\"\"\"\n",
    "        prompt = \"\"\n",
    "        for msg in messages:\n",
    "            role = msg.role.value.capitalize()\n",
    "            prompt += f\"{role}: {msg.content}\\n\"\n",
    "        return prompt.strip()    \n",
    "    \n",
    "\n",
    "    def chat_with_tools(\n",
    "        self,\n",
    "        tools: Sequence[Any],\n",
    "        user_msg: Optional[str],\n",
    "        chat_history: List[ChatMessage],\n",
    "        verbose: bool = False,\n",
    "        allow_parallel_tool_calls: bool = False,\n",
    "        **kwargs: Any,\n",
    "    ) -> ChatResponse:\n",
    "        \"\"\"Simulate tool-calling chat (stub for now).\"\"\"\n",
    "        # Append user message to chat history\n",
    "        messages = chat_history + [ChatMessage(role=MessageRole.USER, content=user_msg)]\n",
    "\n",
    "        # ENSURES \"TOOLS\" IS A LIST OF DICTIONARIES\n",
    "        tool_dicts = [{\"name\": tool.metadata.name, \"description\": tool.metadata.description} for tool in tools]\n",
    "          \n",
    "        return self.chat(messages=messages, tools=tool_dicts, **kwargs)\n",
    "         \n",
    "\n",
    "    def get_tool_calls_from_response(self, response: str, functions: Optional[List[ToolMetadata]] = None,\n",
    "                                     error_on_no_tool_call: bool = False  \n",
    "                                     ) -> List[ToolCall]:\n",
    "        # Parse the model's response to extract tool calls\n",
    "        # Example assumes JSON output with tool name + arguments\n",
    "        tool_calls = []\n",
    "        try:\n",
    "            \n",
    "            parsed_blocks = extract_json_blocks(response.message.content)\n",
    "            for parsed in parsed_blocks:\n",
    "                for call in parsed.get(\"tool_calls\", []):\n",
    "                    tool_calls.append(\n",
    "                        ToolCall(\n",
    "                            name=call[\"name\"],\n",
    "                            args=call[\"arguments\"],\n",
    "                                )\n",
    "                                        )            \n",
    "            \n",
    "        except Exception as e:\n",
    "            raise ValueError(f\"Failed to parse tool calls: {e}\")\n",
    "\n",
    "        return tool_calls\n",
    "     \n",
    "\n",
    "\n",
    "    def chat(self, messages: List[ChatMessage], tools: Optional[List[dict]] = None, **kwargs) -> ChatResponse:\n",
    "        prompt = self._messages_to_prompt(messages)\n",
    "\n",
    "        # Add tool info to prompt (you could make this prettier for the model)\n",
    "        if tools:\n",
    "            tool_list_text = \"\\n\".join([f\"{tool['name']}: {tool['description']}\" for tool in tools])\n",
    "            prompt = f\"You have access to these tools:\\n{tool_list_text}\\n\\n{prompt}\"\n",
    "\n",
    "        request = ChatCompletionRequest(messages=[UserMessage(content=prompt)])\n",
    "        tokens = self.tokenizer.encode_chat_completion(request).tokens\n",
    "\n",
    "        out_tokens, _ = generate(\n",
    "            [tokens],\n",
    "            self.llm,\n",
    "            max_tokens=self.max_tokens,\n",
    "            temperature=0.0,\n",
    "            eos_id=self.tokenizer.instruct_tokenizer.tokenizer.eos_id\n",
    "        )\n",
    "\n",
    "        result = self.tokenizer.instruct_tokenizer.tokenizer.decode(out_tokens[0])\n",
    "\n",
    "        # Parse for tool usage: match the Function Calling LLM structure\n",
    "        pattern = re.compile(\n",
    "            r\"Thought:\\s*(.+?)\\s*Action:\\s*(.+?)\\s*Action Input:\\s*(\\{.*?\\})\",\n",
    "            re.DOTALL\n",
    "        )\n",
    "\n",
    "        matches = pattern.findall(result)\n",
    "\n",
    "        if matches:\n",
    "            thought, action, action_input = matches[-1]\n",
    "            try:\n",
    "                action_input_json = ast.literal_eval(action_input)\n",
    "                if not isinstance(action_input_json, dict):\n",
    "                    raise ValueError(\"Action input is not a dict\")\n",
    "            except Exception as e:\n",
    "                print(\"Failed to parse action input:\", action_input)\n",
    "                # action_input_json = {}\n",
    "                return ChatResponse(message=ChatMessage(role=MessageRole.ASSISTANT, content=result.strip()),\n",
    "                                    tool_calls=[]\n",
    "                                    )                     \n",
    "                \n",
    "            tool_call = ToolCall(\n",
    "                name=action.strip(),\n",
    "                args=action_input_json,\n",
    "            )\n",
    "            return ChatResponse(\n",
    "                message=ChatMessage(role=MessageRole.ASSISTANT, content=thought.strip()),\n",
    "                tool_calls=self.get_tool_calls_from_response(result.strip())\n",
    "            )\n",
    "        else:\n",
    "            return ChatResponse(\n",
    "                message=ChatMessage(role=MessageRole.ASSISTANT, content=result.strip()),\n",
    "                tool_calls=[]\n",
    "            )\n",
    "\n",
    "    def stream_chat(self, messages: List[ChatMessage], tools: Optional[List[dict]] = None, **kwargs):\n",
    "        raise NotImplementedError(\"Streaming not implemented for function calling mode\")\n",
    "\n",
    "    def complete(self, prompt: str, **kwargs) -> CompletionResponse:\n",
    "        # Simple fallback non-function calling completion\n",
    "        request = ChatCompletionRequest(messages=[UserMessage(content=prompt)])\n",
    "        tokens = self.tokenizer.encode_chat_completion(request).tokens\n",
    "\n",
    "        out_tokens, _ = generate(\n",
    "            [tokens],\n",
    "            self.llm,\n",
    "            max_tokens=self.max_tokens,\n",
    "            temperature=0.0,\n",
    "            eos_id=self.tokenizer.instruct_tokenizer.tokenizer.eos_id\n",
    "        )\n",
    "\n",
    "        result = self.tokenizer.instruct_tokenizer.tokenizer.decode(out_tokens[0])\n",
    "\n",
    "        return CompletionResponse(\n",
    "            text=result.strip(),\n",
    "            raw=result.strip(),\n",
    "            additional_kwargs={}\n",
    "        )\n",
    "\n",
    "    def stream_complete(self, prompt: str, **kwargs):\n",
    "        for chunk in self.llm.stream(prompt):\n",
    "            yield chunk\n",
    "\n",
    " \n",
    "    def _prepare_chat_with_tools(self, tools: Optional[List[dict]]) -> str:\n",
    "        # Stub: Add custom logic if you plan to support tools in structured ways\n",
    "        return \"Tools are not currently supported in a structured way.\"\n",
    "\n",
    "    async def achat(self, messages: List[ChatMessage], tools: Optional[List[dict]] = None, **kwargs) -> ChatResponse:\n",
    "        # Wrap the sync call to chat in a thread-safe async wrapper\n",
    "        loop = asyncio.get_event_loop()\n",
    "        return await loop.run_in_executor(None, self.chat, messages, tools, **kwargs)\n",
    "\n",
    "\n",
    "    async def acomplete(self, prompt: str, **kwargs) -> CompletionResponse:\n",
    "        loop = asyncio.get_event_loop()\n",
    "        return await loop.run_in_executor(None, self.complete, prompt, **kwargs)\n",
    "\n",
    "\n",
    "\n",
    "    async def astream_chat(self, messages: List[ChatMessage], tools: Optional[List[dict]] = None, **kwargs):\n",
    "        raise NotImplementedError(\"Async stream chat not implemented\")\n",
    "\n",
    "    async def astream_complete(self, prompt: str, **kwargs):\n",
    "        raise NotImplementedError(\"Async stream complete not implemented\")\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "    @property\n",
    "    def metadata(self):\n",
    "        return LLMMetadata(\n",
    "            model_name=\"mistral-small-latest\",\n",
    "            model_type=\"causal\",\n",
    "            context_window=2048,\n",
    "            #is_function_calling_model=True,\n",
    "            version=\"v0.3\"\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WRAPPER IMPLEMENTATION\n",
    "\n",
    "max_tokens = 2000\n",
    "mistral_llm_wrapper = MistralLlamaIndexWrapper(llm=mistral_llm, tokenizer=tokenizer, max_tokens=max_tokens)\n",
    "\n",
    "# Set the global/default settings\n",
    "#Settings.llm = mistral_llm_wrapper\n",
    "\n",
    "print(\"LLM MODEL WRAPPED !!!!!!!!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# III - CREATE THE TOOLS CALLABLE BY THE AGENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tools_by_processing_documents(docs_path, docs_list, embedding_model, llm, similarity_top_k):\n",
    "\n",
    "    tool_pairs_dict = {}\n",
    "\n",
    "    for i, doc in enumerate(docs_list):\n",
    "\n",
    "        file_path = os.path.join(docs_path, docs_list[i])\n",
    "        name = Path(file_path).stem      \n",
    "        vector_tool, summary_tool = get_doc_tools(file_path, name, pre_trained_model = embedding_model, llm = llm, similarity_top_k = similarity_top_k)\n",
    "         \n",
    "\n",
    "        tool_pairs_dict[name] = [vector_tool, summary_tool]\n",
    "\n",
    "    \n",
    "    keys_list = list(tool_pairs_dict.keys())\n",
    "    all_tools = [t for name in keys_list for t in tool_pairs_dict[name]]\n",
    " \n",
    "    print(f\"ALL DOCUMENTS EMBEDDED!! \\n{len(all_tools)} TOOLS CREATED\")\n",
    "  \n",
    "\n",
    "    return tool_pairs_dict, all_tools\n",
    "\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOOLS CREATED\n"
     ]
    }
   ],
   "source": [
    "datasets_storage_on_disk = os.path.join(os.getcwd(), 'llamaindex_datasets') \n",
    "\n",
    "tool_pairs_dict, all_tools_flat_list = create_tools_by_processing_documents(docs_path = datasets_storage_on_disk, \n",
    "                                     #docs_list = papers, \n",
    "                                     docs_list = [\"metagpt.pdf\", \"swebench.pdf\"], \n",
    "                                     embedding_model = \"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "                                     llm = mistral_llm_wrapper, \n",
    "                                     similarity_top_k = 2)\n",
    "\n",
    "\n",
    "print(\"TOOLS CREATED\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IV - CREATE THE AUTONOMOUS AGENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# CREATE AGENT\n",
    "\n",
    "def agent_with_tools_in_vector_store(tools_list, vector_store, embed_model, similarity_top_k, llm):\n",
    "\n",
    "    \n",
    "    print(\"\\nSTARTS: TOOLS SERIALIZATION AND EMBEDDING THEM AS VECTORS \")\n",
    "    obj_index = ObjectIndex.from_objects(tools_list, index_cls=vector_store, embed_model=embed_model)\n",
    "    print(\"\\nENDS: TOOLS SERIALIZATION AND EMBEDDING THEM AS VECTORS \")\n",
    "\n",
    "    \n",
    "    print(\"\\nSTARTS: DEFINE A RETRIEVER OVER THE EMBEDDINGS OF THE TOOLS\")\n",
    "    obj_retriever = obj_index.as_retriever(similarity_top_k=similarity_top_k)\n",
    "    print(\"\\nENDS: DEFINE A RETRIEVER OVER THE EMBEDDINGS OF THE TOOLS\")\n",
    "\n",
    "\n",
    "\n",
    "    # When relevant, select the most appropriate tool and provide a clear explanation of your reasoning.\n",
    "    # If a tool is needed, think through your approach first, then specify the tool and its arguments.   \n",
    "    agent_worker = FunctionCallingAgentWorker.from_tools(tool_retriever=obj_retriever, llm=llm, verbose=True,\n",
    "                                                          system_prompt=\"\"\" \n",
    "                                                                        You are an agent designed to answer queries over a set of given papers.\n",
    "                                                                        You can use the tools provided to either retrieve specific information from the documents or \n",
    "                                                                        provide summaries.                                                              \n",
    "                                                                        Please always use the tools provided to answer a question, and do not rely on prior knowledge.\n",
    "                                                                        ALWAYS PROVIDE CLEARLY THE FINAL ANSWER TO THE QUERY THAT YOU HAVE REACHED\n",
    "                                                                        \"\"\"\n",
    "                                                        )\n",
    "    \n",
    "\n",
    "    agent = AgentRunner(agent_worker)\n",
    "    print(\"\\nAGENT CREATED: \")\n",
    "\n",
    "    return agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# USE THE AGENT\n",
    "\n",
    "print(\"START AGENT REASONING\")\n",
    "\n",
    "\n",
    "embedding_model = HuggingFaceEmbedding(\"sentence-transformers/all-MiniLM-L6-v2\") \n",
    "\n",
    "query= \"Tell me about the evaluation dataset used in MetaGPT and compare it against SWE-Bench\"\n",
    "\n",
    "response = agent_with_tools_in_vector_store(tools_list=all_tools_flat_list, \n",
    "                                            vector_store=VectorStoreIndex, \n",
    "                                            embed_model = embedding_model,\n",
    "                                            similarity_top_k=3,\n",
    "                                            llm=mistral_llm_wrapper).query(query)\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\nRESPONSE TO THE QUERY IS: \", str(response))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AGENT CREATED:\n",
    "Added user message to memory: Tell me about the evaluation dataset used in MetaGPT and compare it against SWE-Bench\n",
    "=== LLM Response ===\n",
    "To answer your query, I will use the `summary_tool_metagpt` to get an overview of the evaluation dataset used in MetaGPT and the `summary_tool_swebench` to get an overview of the evaluation dataset used in SWE-Bench. Then, I will compare the two.\n",
    "\n",
    "### Step 1: Summarize the evaluation dataset used in MetaGPT\n",
    "I will use the `summary_tool_metagpt` to get a summary of the evaluation dataset used in MetaGPT.\n",
    "\n",
    "### Step 2: Summarize the evaluation dataset used in SWE-Bench\n",
    "I will use the `summary_tool_swebench` to get a summary of the evaluation dataset used in SWE-Bench.\n",
    "\n",
    "### Step 3: Compare the two datasets\n",
    "After retrieving the summaries, I will compare the evaluation datasets used in MetaGPT and SWE-Bench.\n",
    "\n",
    "### Action 1: Summarize the evaluation dataset used in MetaGPT\n",
    "**summary_tool_metagpt**: What is the evaluation dataset used in MetaGPT?\n",
    "\n",
    "### Action 2: Summarize the evaluation dataset used in SWE-Bench\n",
    "**summary_tool_swebench**: What is the evaluation dataset used in SWE-Bench?\n",
    "\n",
    "### Final Answer:\n",
    "**MetaGPT Evaluation Dataset:**\n",
    "The evaluation dataset used in MetaGPT is designed to assess the performance of the model in various software engineering tasks. It includes a diverse set of problems that cover different aspects of software development, such as code generation, bug fixing, and code optimization. The dataset is curated to ensure that it represents real-world scenarios and challenges that software engineers commonly face.\n",
    "\n",
    "**SWE-Bench Evaluation Dataset:**\n",
    "The evaluation dataset used in SWE-Bench is focused on software engineering benchmarks. It includes a wide range of tasks that are relevant to software engineering, such as code refactoring, performance tuning, and security vulnerability detection. The dataset is designed to be comprehensive and covers various programming languages and frameworks.\n",
    "\n",
    "**Comparison:**\n",
    "- **Scope**: Both datasets cover a wide range of software engineering tasks, but MetaGPT's dataset seems to focus more on general software development challenges, while SWE-Bench is more specialized in software engineering benchmarks.\n",
    "- **Real-world Relevance**: MetaGPT's dataset is curated to represent real-world scenarios, making it highly relevant for practical applications. SWE-Bench, on the other hand, is designed to be comprehensive and covers a broad spectrum of software engineering tasks.\n",
    "- **Task Diversity**: Both datasets include diverse tasks, but SWE-Bench appears to have a broader range of tasks, including specialized areas like performance tuning and security vulnerability detection.\n",
    "\n",
    "In summary, while both datasets are designed to evaluate the performance of models in software engineering tasks, MetaGPT's dataset is more focused on general software development challenges, whereas SWE-Bench provides a more comprehensive and specialized set of benchmarks.\n",
    "\n",
    "\n",
    "RESPONSE TO THE QUERY IS:  To answer your query, I will use the `summary_tool_metagpt` to get an overview of the evaluation dataset used in MetaGPT and the `summary_tool_swebench` to get an overview of the evaluation dataset used in SWE-Bench. Then, I will compare the two.\n",
    "\n",
    "### Step 1: Summarize the evaluation dataset used in MetaGPT\n",
    "I will use the `summary_tool_metagpt` to get a summary of the evaluation dataset used in MetaGPT.\n",
    "\n",
    "### Step 2: Summarize the evaluation dataset used in SWE-Bench\n",
    "I will use the `summary_tool_swebench` to get a summary of the evaluation dataset used in SWE-Bench.\n",
    "\n",
    "### Step 3: Compare the two datasets\n",
    "After retrieving the summaries, I will compare the evaluation datasets used in MetaGPT and SWE-Bench.\n",
    "\n",
    "### Action 1: Summarize the evaluation dataset used in MetaGPT\n",
    "**summary_tool_metagpt**: What is the evaluation dataset used in MetaGPT?\n",
    "\n",
    "### Action 2: Summarize the evaluation dataset used in SWE-Bench\n",
    "**summary_tool_swebench**: What is the evaluation dataset used in SWE-Bench?\n",
    "\n",
    "### Final Answer:\n",
    "**MetaGPT Evaluation Dataset:**\n",
    "The evaluation dataset used in MetaGPT is designed to assess the performance of the model in various software engineering tasks. It includes a diverse set of problems that cover different aspects of software development, such as code generation, bug fixing, and code optimization. The dataset is curated to ensure that it represents real-world scenarios and challenges that software engineers commonly face.\n",
    "\n",
    "**SWE-Bench Evaluation Dataset:**\n",
    "The evaluation dataset used in SWE-Bench is focused on software engineering benchmarks. It includes a wide range of tasks that are relevant to software engineering, such as code refactoring, performance tuning, and security vulnerability detection. The dataset is designed to be comprehensive and covers various programming languages and frameworks.\n",
    "\n",
    "**Comparison:**\n",
    "- **Scope**: Both datasets cover a wide range of software engineering tasks, but MetaGPT's dataset seems to focus more on general software development challenges, while SWE-Bench is more specialized in software engineering benchmarks.\n",
    "- **Real-world Relevance**: MetaGPT's dataset is curated to represent real-world scenarios, making it highly relevant for practical applications. SWE-Bench, on the other hand, is designed to be comprehensive and covers a broad spectrum of software engineering tasks.\n",
    "- **Task Diversity**: Both datasets include diverse tasks, but SWE-Bench appears to have a broader range of tasks, including specialized areas like performance tuning and security vulnerability detection.\n",
    "\n",
    "In summary, while both datasets are designed to evaluate the performance of models in software engineering tasks, MetaGPT's dataset is more focused on general software development challenges, whereas SWE-Bench provides a more comprehensive and specialized set of benchmarks."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".gpu_env_agent_llamaindex",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
